{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e2fb31f127fed1e79f709faf34890f85681c06e7e30d28a328e5c7500cdf6725"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "source": [
    "# 1. Linear Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "y_train: (50000, 1)\n",
      "x_test: (10000, 32, 32, 3)\n",
      "y_test: (10000, 1) \n",
      "\n",
      "x_train reshaped: (50000, 3072)\n",
      "y_train reshaped: (50000, 10)\n",
      "x_test reshaped: (10000, 3072)\n",
      "y_test reshaped: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:',y_train.shape)\n",
    "print('x_test:',x_test.shape)\n",
    "print('y_test:',y_test.shape,'\\n')\n",
    "\n",
    "# Classes\n",
    "K = len(np.unique(y_train)) \n",
    "\n",
    "# Data Parameters\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "# Covert labels to binary classes\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train reshaped:', x_train.shape)\n",
    "print('y_train reshaped:',y_train.shape)\n",
    "print('x_test reshaped:',x_test.shape)\n",
    "print('y_test reshaped:',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w1: (3072, 10)\nb1: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, K)\n",
    "b1 = np.zeros((1,K))\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr\n",
    "\n",
    "# Initialize parameters for linear model\n",
    "iterations = 300\n",
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6 # Regularization parameter - Lambda\n",
    "\n",
    "# Initialize lists to store history\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "source": [
    "### Functions - Linear Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLoss(y_p,Y,w1,reg):\n",
    "    # Calculates loss using the mean sum of squared errors\n",
    "    dy = y_p - Y # Difference\n",
    "    batch_size = y_p.shape[0]\n",
    "    return (1/batch_size)*np.sum(np.square(dy)) + np.sum(w1**2)*reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardProp(X,w1,b1):\n",
    "    # Implements the forward pass of the linear model\n",
    "    return np.matmul(X,w1)+b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackProp(X,Y,y_p):\n",
    "    # Calculates dw1 and db1\n",
    "    dy = 2*(y_p-Y)/X.shape[0] # Derivative\n",
    "    dw = np.matmul(X.T,dy) ; db = np.sum(dy,axis=0)\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradDescent(w1,b1,dw,db,lr,lr_decay):\n",
    "    # Updates weights and learning rate\n",
    "    w1 -= dw*lr ;  b1 -= db*lr ; lr *= lr_decay\n",
    "    return w1,b1,lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(Y,y_p):\n",
    "    # Returns a similarity figure of accuracy\n",
    "    y_p_in = np.argmax(y_p,axis=1)\n",
    "    y_class = np.argmax(Y,axis=1)\n",
    "    return np.sum(y_p_in==y_class)*100/y_class.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration : 1 ---------- loss 0.9578989982211862\n",
      "Iteration : 11 ---------- loss 0.8721104849742985\n",
      "Iteration : 21 ---------- loss 0.8401459385261969\n",
      "Iteration : 31 ---------- loss 0.8222232694884302\n",
      "Iteration : 41 ---------- loss 0.8115404866693534\n",
      "Iteration : 51 ---------- loss 0.804886157791459\n",
      "Iteration : 61 ---------- loss 0.8005452032479465\n",
      "Iteration : 71 ---------- loss 0.797568080538036\n",
      "Iteration : 81 ---------- loss 0.7954177422692429\n",
      "Iteration : 91 ---------- loss 0.7937847285349873\n",
      "Iteration : 101 ---------- loss 0.7924872755938612\n",
      "Iteration : 111 ---------- loss 0.7914163414431038\n",
      "Iteration : 121 ---------- loss 0.7905049109487765\n",
      "Iteration : 131 ---------- loss 0.7897106368710003\n",
      "Iteration : 141 ---------- loss 0.7890059019124888\n",
      "Iteration : 151 ---------- loss 0.7883720582186725\n",
      "Iteration : 161 ---------- loss 0.7877960439864236\n",
      "Iteration : 171 ---------- loss 0.7872683671530436\n",
      "Iteration : 181 ---------- loss 0.7867818838846856\n",
      "Iteration : 191 ---------- loss 0.7863310445122811\n",
      "Iteration : 201 ---------- loss 0.7859114178847877\n",
      "Iteration : 211 ---------- loss 0.7855193839163765\n",
      "Iteration : 221 ---------- loss 0.7851519293903848\n",
      "Iteration : 231 ---------- loss 0.7848065083314973\n",
      "Iteration : 241 ---------- loss 0.7844809436072068\n",
      "Iteration : 251 ---------- loss 0.784173355480059\n",
      "Iteration : 261 ---------- loss 0.7838821082346434\n",
      "Iteration : 271 ---------- loss 0.7836057692607109\n",
      "Iteration : 281 ---------- loss 0.7833430769624457\n",
      "Iteration : 291 ---------- loss 0.7830929150951859\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    # Forward pass\n",
    "    X = x_train[indices]\n",
    "    Y = y_train[indices]\n",
    "    \n",
    "    y_p = ForwardProp(X,w1,b1)\n",
    "\n",
    "    # Calculate training loss for each iteration\n",
    "    loss = LinearLoss(y_p,Y,w1,reg)\n",
    "\n",
    "    if not((t-1)%10):print('Iteration :',t,'---------- loss',loss)\n",
    "    \n",
    "    # Update history\n",
    "    loss_history.append(loss)\n",
    "    train_acc_history.append(Accuracy(Y,y_p))\n",
    "\n",
    "    # Backward pass\n",
    "    dw,db = BackProp(X,Y,y_p)\n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    w1,b1,lr = GradDescent(w1,b1,dw,db,lr,lr_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "41.942"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# Printing accuracies and displaying w as images\n",
    "\n",
    "train_acc_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy : 41.952\n"
     ]
    }
   ],
   "source": [
    "# Train Accuracy\n",
    "\n",
    "    print('Train accuracy :',np.sum(y_p_in==y_class)*100/y_class.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.010369898450185398\nTraining set loss: 0.7829008427980855\nTest set loss: 0.7876065250735699\nTest Accuracy: 40.57\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "y_pred = np.matmul(x_test,w1)+b1\n",
    "\n",
    "print(\"Learning rate:\", lr)\n",
    "print(\"Training set loss:\", loss_history[-1])\n",
    "\n",
    "loss_test = (1/Nte)*np.sum(np.square(y_pred - y_test)) + np.sum(w1**2)*reg\n",
    "print(\"Test set loss:\", loss_test)\n",
    "\n",
    "y_pred_in = np.argmax(y_pred,axis=1)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(\"Test Accuracy:\", np.sum(y_pred_in==y_test_class)*100/y_test_class.size)"
   ]
  },
  {
   "source": [
    "# 2. 2-Layer Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w1: (3072, 200)\nb1: (1, 200)\nw2: (200, 10)\nb2: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "H = 200\n",
    "w_1 = std*np.random.randn(Din, H)\n",
    "b_1 = np.zeros((1,H))\n",
    "w_2 = std*np.random.randn(H, K)\n",
    "b_2 = np.zeros((1,K))\n",
    "\n",
    "print(\"w1:\", w_1.shape)\n",
    "print(\"b1:\", b_1.shape)\n",
    "print(\"w2:\", w_2.shape)\n",
    "print(\"b2:\", b_2.shape)\n",
    "\n",
    "iterations = 300\n",
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6\n",
    "loss_history_nn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : 0 ---------- loss 0.9999533676809134\n",
      "t : 10 ---------- loss 0.899999996033859\n",
      "t : 20 ---------- loss 0.8999999955213841\n",
      "t : 30 ---------- loss 0.8999999950141696\n",
      "t : 40 ---------- loss 0.8999999945120051\n",
      "t : 50 ---------- loss 0.8999999940148401\n",
      "t : 60 ---------- loss 0.8999999935226263\n",
      "t : 70 ---------- loss 0.8999999930353132\n",
      "t : 80 ---------- loss 0.8999999925528522\n",
      "t : 90 ---------- loss 0.8999999920751947\n",
      "t : 100 ---------- loss 0.8999999916022934\n",
      "t : 110 ---------- loss 0.8999999911341006\n",
      "t : 120 ---------- loss 0.8999999906705691\n",
      "t : 130 ---------- loss 0.899999990211653\n",
      "t : 140 ---------- loss 0.8999999897573061\n",
      "t : 150 ---------- loss 0.8999999893074826\n",
      "t : 160 ---------- loss 0.8999999888621376\n",
      "t : 170 ---------- loss 0.8999999884212269\n",
      "t : 180 ---------- loss 0.8999999879847057\n",
      "t : 190 ---------- loss 0.8999999875525305\n",
      "t : 200 ---------- loss 0.8999999871246576\n",
      "t : 210 ---------- loss 0.8999999867010456\n",
      "t : 220 ---------- loss 0.8999999862816503\n",
      "t : 230 ---------- loss 0.8999999858664309\n",
      "t : 240 ---------- loss 0.8999999854553447\n",
      "t : 250 ---------- loss 0.8999999850483514\n",
      "t : 260 ---------- loss 0.8999999846454103\n",
      "t : 270 ---------- loss 0.8999999842464804\n",
      "t : 280 ---------- loss 0.8999999838515219\n",
      "t : 290 ---------- loss 0.8999999834604958\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    Xi = x_train[indices]\n",
    "    Yi = y_train[indices]\n",
    "    \n",
    "    ac = sigmoid(np.matmul(Xi,w_1)+b_1)\n",
    "    y_nn = np.matmul(ac,w_2)+b_2\n",
    "    dy = y_nn - Yi\n",
    "    loss = (1/batch_size)*np.sum(np.square(dy)) + np.sum(w_1**2)*reg\n",
    "    loss_history_nn.append(loss)\n",
    "    if not(t%10):print('t :',t,'---------- loss',loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dac = np.matmul(2*dy,w_2.T)/batch_size\n",
    "    dw1 = np.matmul(Xi.T,dac*ac*(1-ac))/batch_size + 2*w_1*reg\n",
    "    db1 = np.sum(dac*ac*(1-ac),axis=0)/batch_size  \n",
    "\n",
    "    dw2 = np.matmul(ac.T,2*dy)/batch_size + 2*w_2*reg\n",
    "    db2 = np.sum(2*dy,axis=0)/batch_size  \n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    w_1 -= dw1*lr\n",
    "    b_1 -= db1*lr\n",
    "    w_2 -= dw2*lr\n",
    "    b_2 -= db2*lr\n",
    "    lr *= lr_decay    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy : 10.054\n"
     ]
    }
   ],
   "source": [
    "# Train Accuracy\n",
    "\n",
    "y_p_in = np.argmax(y_nn,axis=1)\n",
    "y_class = np.argmax(Yi,axis=1)\n",
    "print('Train accuracy :',np.sum(y_p_in==y_class)*100/y_class.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.005433502828270419\nTraining set loss: 0.8999999823415467\nTest set loss: 0.8999999823487368\nTest Accuracy: 10.05\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "ac_t = sigmoid(np.matmul(x_test,w_1)+b_1)\n",
    "y_predic = np.matmul(ac_t,w_2)+b_2\n",
    "\n",
    "print(\"Learning rate:\", lr)\n",
    "print(\"Training set loss:\", loss_history[-1])\n",
    "\n",
    "loss_test = (1/Nte)*np.sum(np.square(y_predic - y_test))\n",
    "print(\"Test set loss:\", loss_test)\n",
    "\n",
    "y_predic_in = np.argmax(y_predic,axis=1)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(\"Test Accuracy:\", np.sum(y_predic_in==y_test_class)*100/y_test_class.size)"
   ]
  },
  {
   "source": [
    "# 3. Stochastic Gradient Descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w1: (3072, 200)\nb1: (1, 200)\nw2: (200, 10)\nb2: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "w_1 = std*np.random.randn(Din, H)\n",
    "b_1 = np.zeros((1,H))\n",
    "w_2 = std*np.random.randn(H, K)\n",
    "b_2 = np.zeros((1,K))\n",
    "\n",
    "print(\"w1:\", w_1.shape)\n",
    "print(\"b1:\", b_1.shape)\n",
    "print(\"w2:\", w_2.shape)\n",
    "print(\"b2:\", b_2.shape)\n",
    "\n",
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6\n",
    "loss_history_st = []\n",
    "\n",
    "batch_size = 500\n",
    "per_iter = Ntr//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : 0 ---------- loss 0.9008984393065269\n",
      "t : 10 ---------- loss 0.9010242749428756\n",
      "t : 20 ---------- loss 0.8998755183687912\n",
      "t : 30 ---------- loss 0.9001436348757059\n",
      "t : 40 ---------- loss 0.9002109543043372\n",
      "t : 50 ---------- loss 0.8999962857309703\n",
      "t : 60 ---------- loss 0.9000051150120449\n",
      "t : 70 ---------- loss 0.9000034952564726\n",
      "t : 80 ---------- loss 0.9000015178841494\n",
      "t : 90 ---------- loss 0.9000015639762128\n",
      "t : 100 ---------- loss 0.8999994476612927\n",
      "t : 110 ---------- loss 0.8999997281218244\n",
      "t : 120 ---------- loss 0.8999998851480435\n",
      "t : 130 ---------- loss 0.8999995449923835\n",
      "t : 140 ---------- loss 0.9000002313405648\n",
      "t : 150 ---------- loss 0.899999506711902\n",
      "t : 160 ---------- loss 0.8999999388771533\n",
      "t : 170 ---------- loss 0.9000000761329955\n",
      "t : 180 ---------- loss 0.9000003026589055\n",
      "t : 190 ---------- loss 0.8999995463692197\n",
      "t : 200 ---------- loss 0.9000002005616976\n",
      "t : 210 ---------- loss 0.9000007463238083\n",
      "t : 220 ---------- loss 0.8999996852648459\n",
      "t : 230 ---------- loss 0.8999992903861633\n",
      "t : 240 ---------- loss 0.9000000070750801\n",
      "t : 250 ---------- loss 0.8999986732694982\n",
      "t : 260 ---------- loss 0.9000000998098459\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-175-8534820bac84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Perform Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mw_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mdw1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mb_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mdb1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mw_2\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mdw2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    Xs = x_train[indices]\n",
    "    Ys = y_train[indices]\n",
    "\n",
    "    for iter in range(per_iter):\n",
    "        batch = Xs[batch_size*iter:batch_size*(iter+1)]\n",
    "        batchy = Ys[batch_size*iter:batch_size*(iter+1)]    \n",
    "        ac = sigmoid(np.matmul(batch,w_1)+b_1)\n",
    "        y_b = np.matmul(ac,w_2)+b_2\n",
    "        dy = y_b - batchy\n",
    "        loss = (1/batch_size)*np.sum(np.square(dy)) + np.sum(w_1**2)*reg\n",
    "        loss_history_st.append(loss)\n",
    "\n",
    "    \n",
    "        # Backward pass\n",
    "        dac = np.matmul(2*dy,w_2.T)/batch_size\n",
    "        dw1 = np.matmul(batch.T,dac*ac*(1-ac))/batch_size + 2*w_1*reg\n",
    "        db1 = np.sum(dac*ac*(1-ac),axis=0)/batch_size  \n",
    "\n",
    "        dw2 = np.matmul(ac.T,2*dy)/batch_size + 2*w_2*reg\n",
    "        db2 = np.sum(2*dy,axis=0)/batch_size  \n",
    "\n",
    "        # Perform Gradient Descent\n",
    "        w_1 -= dw1*lr\n",
    "        b_1 -= db1*lr\n",
    "        w_2 -= dw2*lr\n",
    "        b_2 -= db2*lr\n",
    "        lr *= lr_decay  \n",
    "\n",
    "    if not(t%10):print('t :',t,'---------- loss',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "dy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}