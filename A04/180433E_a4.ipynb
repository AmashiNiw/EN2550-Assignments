{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e2fb31f127fed1e79f709faf34890f85681c06e7e30d28a328e5c7500cdf6725"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Linear Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train: (50000, 32, 32, 3)\n",
      "w1: (3072, 10)\n",
      "b1: (1, 10)\n",
      "t : 0 ---------- loss 1.0000412910238619\n",
      "t : 10 ---------- loss 0.876636905454685\n",
      "t : 20 ---------- loss 0.8425399261678642\n",
      "t : 30 ---------- loss 0.8236159816614267\n",
      "t : 40 ---------- loss 0.8123886621188027\n",
      "t : 50 ---------- loss 0.8054259349351952\n",
      "t : 60 ---------- loss 0.8009055721657667\n",
      "t : 70 ---------- loss 0.7978212855921176\n",
      "t : 80 ---------- loss 0.7956050261531737\n",
      "t : 90 ---------- loss 0.7939300752120351\n",
      "t : 100 ---------- loss 0.7926049121525146\n",
      "t : 110 ---------- loss 0.7915148984484983\n",
      "t : 120 ---------- loss 0.7905897606411252\n",
      "t : 130 ---------- loss 0.7897852235860183\n",
      "t : 140 ---------- loss 0.7890725083544458\n",
      "t : 150 ---------- loss 0.7884322524252738\n",
      "t : 160 ---------- loss 0.7878509436404073\n",
      "t : 170 ---------- loss 0.7873187982945276\n",
      "t : 180 ---------- loss 0.7868284778916006\n",
      "t : 190 ---------- loss 0.78637429855885\n",
      "t : 200 ---------- loss 0.7859517335043814\n",
      "t : 210 ---------- loss 0.7855570922393196\n",
      "t : 220 ---------- loss 0.7851873081326131\n",
      "t : 230 ---------- loss 0.7848397935756984\n",
      "t : 240 ---------- loss 0.7845123382231433\n",
      "t : 250 ---------- loss 0.7842030353221011\n",
      "t : 260 ---------- loss 0.7839102268297846\n",
      "t : 270 ---------- loss 0.7836324614427537\n",
      "t : 280 ---------- loss 0.7833684617497984\n",
      "t : 290 ---------- loss 0.7831170980110181\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train:', x_train.shape)\n",
    "\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, K)\n",
    "b1 = np.zeros((1,K))\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "batch_size = Ntr\n",
    "\n",
    "iterations = 300\n",
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "    \n",
    "    # Forward pass\n",
    "    X = x_train[indices]\n",
    "    Y = y_train[indices]\n",
    "    \n",
    "    y_p = np.matmul(X,w1)+b1\n",
    "    dy = y_p - Y\n",
    "    loss = (1/batch_size)*np.sum(np.square(dy)) + np.sum(w1**2)*reg\n",
    "    loss_history.append(loss)\n",
    "    if not(t%10):print('t :',t,'---------- loss',loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dw = np.matmul(X.T,2*dy)/batch_size  \n",
    "    db = np.sum(2*dy,axis=0)/batch_size  \n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    w1 -= dw*lr\n",
    "    b1 -= db*lr\n",
    "    lr *= lr_decay\n",
    "# Printing accuracies and displaying w as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy : 41.952\n"
     ]
    }
   ],
   "source": [
    "# Train Accuracy\n",
    "\n",
    "y_p_in = np.argmax(y_p,axis=1)\n",
    "y_class = np.argmax(Y,axis=1)\n",
    "print('Train accuracy :',np.sum(y_p_in==y_class)*100/y_class.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.010369898450185398\nTraining set loss: 0.7829008427980855\nTest set loss: 0.7876065250735699\nTest Accuracy: 40.57\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "y_pred = np.matmul(x_test,w1)+b1\n",
    "\n",
    "print(\"Learning rate:\", lr)\n",
    "print(\"Training set loss:\", loss_history[-1])\n",
    "\n",
    "loss_test = (1/Nte)*np.sum(np.square(y_pred - y_test)) + np.sum(w1**2)*reg\n",
    "print(\"Test set loss:\", loss_test)\n",
    "\n",
    "y_pred_in = np.argmax(y_pred,axis=1)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(\"Test Accuracy:\", np.sum(y_pred_in==y_test_class)*100/y_test_class.size)"
   ]
  },
  {
   "source": [
    "# 2. 2-Layer Neural Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w1: (3072, 200)\nb1: (1, 200)\nw2: (200, 10)\nb2: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "H = 200\n",
    "w_1 = std*np.random.randn(Din, H)\n",
    "b_1 = np.zeros((1,H))\n",
    "w_2 = std*np.random.randn(H, K)\n",
    "b_2 = np.zeros((1,K))\n",
    "\n",
    "print(\"w1:\", w_1.shape)\n",
    "print(\"b1:\", b_1.shape)\n",
    "print(\"w2:\", w_2.shape)\n",
    "print(\"b2:\", b_2.shape)\n",
    "\n",
    "iterations = 300\n",
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6\n",
    "loss_history_nn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t : 0 ---------- loss 0.9999533676809134\n",
      "t : 10 ---------- loss 0.899999996033859\n",
      "t : 20 ---------- loss 0.8999999955213841\n",
      "t : 30 ---------- loss 0.8999999950141696\n",
      "t : 40 ---------- loss 0.8999999945120051\n",
      "t : 50 ---------- loss 0.8999999940148401\n",
      "t : 60 ---------- loss 0.8999999935226263\n",
      "t : 70 ---------- loss 0.8999999930353132\n",
      "t : 80 ---------- loss 0.8999999925528522\n",
      "t : 90 ---------- loss 0.8999999920751947\n",
      "t : 100 ---------- loss 0.8999999916022934\n",
      "t : 110 ---------- loss 0.8999999911341006\n",
      "t : 120 ---------- loss 0.8999999906705691\n",
      "t : 130 ---------- loss 0.899999990211653\n",
      "t : 140 ---------- loss 0.8999999897573061\n",
      "t : 150 ---------- loss 0.8999999893074826\n",
      "t : 160 ---------- loss 0.8999999888621376\n",
      "t : 170 ---------- loss 0.8999999884212269\n",
      "t : 180 ---------- loss 0.8999999879847057\n",
      "t : 190 ---------- loss 0.8999999875525305\n",
      "t : 200 ---------- loss 0.8999999871246576\n",
      "t : 210 ---------- loss 0.8999999867010456\n",
      "t : 220 ---------- loss 0.8999999862816503\n",
      "t : 230 ---------- loss 0.8999999858664309\n",
      "t : 240 ---------- loss 0.8999999854553447\n",
      "t : 250 ---------- loss 0.8999999850483514\n",
      "t : 260 ---------- loss 0.8999999846454103\n",
      "t : 270 ---------- loss 0.8999999842464804\n",
      "t : 280 ---------- loss 0.8999999838515219\n",
      "t : 290 ---------- loss 0.8999999834604958\n"
     ]
    }
   ],
   "source": [
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    Xi = x_train[indices]\n",
    "    Yi = y_train[indices]\n",
    "    \n",
    "    ac = sigmoid(np.matmul(Xi,w_1)+b_1)\n",
    "    y_nn = np.matmul(ac,w_2)+b_2\n",
    "    dy = y_nn - Yi\n",
    "    loss = (1/batch_size)*np.sum(np.square(dy)) + np.sum(w_1**2)*reg\n",
    "    loss_history_nn.append(loss)\n",
    "    if not(t%10):print('t :',t,'---------- loss',loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    dac = np.matmul(2*dy,w_2.T)/batch_size\n",
    "    dw1 = np.matmul(Xi.T,dac*ac*(1-ac))/batch_size + 2*w_1*reg\n",
    "    db1 = np.sum(dac*ac*(1-ac),axis=0)/batch_size  \n",
    "\n",
    "    dw2 = np.matmul(ac.T,2*dy)/batch_size + 2*w_2*reg\n",
    "    db2 = np.sum(2*dy,axis=0)/batch_size  \n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    w_1 -= dw1*lr\n",
    "    b_1 -= db1*lr\n",
    "    w_2 -= dw2*lr\n",
    "    b_2 -= db2*lr\n",
    "    lr *= lr_decay    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train accuracy : 10.054\n"
     ]
    }
   ],
   "source": [
    "# Train Accuracy\n",
    "\n",
    "y_p_in = np.argmax(y_nn,axis=1)\n",
    "y_class = np.argmax(Yi,axis=1)\n",
    "print('Train accuracy :',np.sum(y_p_in==y_class)*100/y_class.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learning rate: 0.005433502828270419\nTraining set loss: 0.8999999823415467\nTest set loss: 0.8999999823487368\nTest Accuracy: 10.05\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "ac_t = sigmoid(np.matmul(x_test,w_1)+b_1)\n",
    "y_predic = np.matmul(ac_t,w_2)+b_2\n",
    "\n",
    "print(\"Learning rate:\", lr)\n",
    "print(\"Training set loss:\", loss_history[-1])\n",
    "\n",
    "loss_test = (1/Nte)*np.sum(np.square(y_predic - y_test))\n",
    "print(\"Test set loss:\", loss_test)\n",
    "\n",
    "y_predic_in = np.argmax(y_predic,axis=1)\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(\"Test Accuracy:\", np.sum(y_predic_in==y_test_class)*100/y_test_class.size)"
   ]
  },
  {
   "source": [
    "# 3. Stochastic Gradient Descent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.014\n",
    "lr_decay = 0.999\n",
    "reg = 5e-6\n",
    "loss_history_st = []\n",
    "\n",
    "batch_size = 500\n",
    "per_iter = Ntr/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    Xi = x_train[indices]\n",
    "    Yi = y_train[indices]"
   ]
  }
 ]
}